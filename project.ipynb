{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib.image import imread\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage import io\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! mkdir data\n",
    "    \n",
    "    # train dataset\n",
    "    # ! wget http://data.csail.mit.edu/places/places365/train_256_places365standard.tar\n",
    "    # ! tar -xvf train_256_places365standard.tar -C data\n",
    "    \n",
    "    # validation dataset\n",
    "    ! wget http://data.csail.mit.edu/places/places365/val_256.tar\n",
    "    ! tar -xvf val_256.tar -C data\n",
    "    \n",
    "    # test dataset\n",
    "    # ! wget http://data.csail.mit.edu/places/places365/test_256.tar\n",
    "    # ! tar -xvf test_256.tar -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/val_256')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"data/\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = np.array(files)\n",
    "        self.length = len(files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = imread(self.files[idx])\n",
    "        if img.shape == (256, 256, 3):  # if a color image\n",
    "            img_lab = rgb2lab(img)\n",
    "            img_lab = (img_lab + [0, 128, 128]) / [100, 255, 255]  # normalize L, a, b dimensions\n",
    "            img_lightness = img_lab[:, :, 0:1].transpose(2, 0, 1)\n",
    "            img_ab = img_lab[:, :, 1:3].transpose(2, 0, 1)\n",
    "        else:  # if a grayscale image\n",
    "            img_lightness = (img/255)[None, :, :]\n",
    "            img_ab = np.zeros(shape=(2, 256, 256))\n",
    "        return img_lightness, img_ab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_files = glob.glob(\"data/val_256/*.jpg\")\n",
    "len(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = val_files[:29200], val_files[29200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageDataset(train)\n",
    "val_ds = ImageDataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_block(in_, out_, kernel_size=3, stride=1):\n",
    "    \"\"\"Return a block consisting of a conv2d, ReLU and BatchNorm2d layer.\"\"\"\n",
    "    padding = kernel_size // 2\n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(in_, out_, kernel_size, stride, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(out_))\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            basic_block(1, 64, kernel_size=3, stride=2),\n",
    "            basic_block(64, 128, kernel_size=3, stride=2),\n",
    "            basic_block(128, 256, kernel_size=3, stride=2),\n",
    "            basic_block(256, 512, kernel_size=3, stride=1),\n",
    "            basic_block(512, 256, kernel_size=3, stride=1))\n",
    "        self.upsample = nn.Upsample(scale_factor=8)\n",
    "        self.out_layer = basic_block(256, 2, kernel_size=3, stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.upsample(x)\n",
    "        return self.out_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path): torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path): model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dl, val_dl, optimizer, epochs=10):\n",
    "    iterations = len(train_dl) * epochs\n",
    "    pbar = tqdm(total=iterations)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        for x, y in train_dl:\n",
    "            x = x.float().cuda()\n",
    "            y = y.float().cuda()\n",
    "            y_hat = model(x)\n",
    "            loss = F.mse_loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * y.shape[0]\n",
    "            total += y.shape[0]\n",
    "            pbar.update()\n",
    "        val_loss = val_metrics(model, val_dl)\n",
    "        print(f\"train loss: {total_loss/total:.8f}\\tval loss: {val_loss:.8f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            path = f\"models/model_{best_val_loss:.6f}\"\n",
    "            save_model(model, path)\n",
    "            print(path)\n",
    "    \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, val_dl):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    for x, y in val_dl:\n",
    "        x = x.float().cuda()\n",
    "        y = y.float()\n",
    "        out = model(x)\n",
    "        loss = F.mse_loss(out, y.cuda())\n",
    "        total_loss += loss.item() * y.shape[0]\n",
    "        total += y.shape[0]\n",
    "        y_pred.append(out.cpu().detach().numpy())\n",
    "        y_true.append(y)\n",
    "    \n",
    "    y_pred = np.vstack(y_pred)\n",
    "    y_true = np.vstack(y_true)\n",
    "    return total_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ColorizationNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6243f8b09c484a32aba09d07be625fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1948.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.04738433\tval loss: 0.00348357\n",
      "models/model_0.003484\n",
      "train loss: 0.00343976\tval loss: 0.00339602\n",
      "models/model_0.003396\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "best_val = train_epoch(model, train_dl, val_dl, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
